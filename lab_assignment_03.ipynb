{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Revanthch/INFO-5502/blob/main/lab_assignment_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1m9CG70z-_R"
      },
      "source": [
        "## The third Lab-assignment (07/22/2022 11:59'AM' - 07/26/2022 11:59PM, 50 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiA9SN9jz-_V"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0kmvlGaz-_W"
      },
      "source": [
        "Question 1 (10 points). Fomulate your domain problem: Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29hi5DjNz-_W"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "\n",
        "Domain problem: My domain problem is to analyse the data that is obtained from the super market sales.\n",
        "\n",
        "Research question: . In order to work on the topic the super market sales. . We need to take the EDA on the City, Gender, Customer Type, Quantity.\n",
        "\n",
        "Data Required to answer the questions: The following are the attributes of the required data to answer the research question\n",
        "\n",
        "Invoice , City , Payment Method , Gender etc.\n",
        "Entire Report of the super market sales.\n",
        "We have 1001 samples of the data from the following link: https://github.com/revanthch/INFO-5502/blob/main/supermarket_sales%20-%20Sheet1.csv\n",
        "\n",
        "Steps for collecting and saving the data:\n",
        "\n",
        "We use web scraping technique to scrape the data\n",
        "We also used autoscraping software which is installed by using pip.\n",
        "We also used autoscraping software which is installed by using pip.\n",
        "There are 1001 rows of data.\n",
        "\n",
        "We go through each page and go to the each sale done in that report.\n",
        "\n",
        "We get the customer type in that report.\n",
        "\n",
        "We go through the Sales reports and all the elements in it and store the text data in a json format.\n",
        "\n",
        "The csv format is the prepared data for our research.\n",
        "\n",
        "\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW7dtjPtz-_Y"
      },
      "source": [
        "Question 2 (30 points). Collect your data to answer the research problem: Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install autoscraper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKOK_Cbj12Op",
        "outputId": "00038db4-f6cb-4439-f199-4516bf1584aa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting autoscraper\n",
            "  Downloading autoscraper-1.1.14-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from autoscraper) (4.2.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autoscraper) (2.23.0)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (from autoscraper) (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4->autoscraper) (4.6.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autoscraper) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autoscraper) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autoscraper) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autoscraper) (2.10)\n",
            "Installing collected packages: autoscraper\n",
            "Successfully installed autoscraper-1.1.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BSGfbCVz-_Y"
      },
      "outputs": [],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "\n",
        "from autoscraper import AutoScraper\n",
        "\n",
        "url = 'https://github.com/revanthch/INFO-5502/blob/main/supermarket_sales%20-%20Sheet1.csv'\n",
        "\n",
        "# We can add one or multiple candidates here.\n",
        "# You can also put urls here to retrieve urls.\n",
        "wanted_list = [\"Customer type\",\"Member\",\"7\",\"Quantity\"]\n",
        "\n",
        "scraper = AutoScraper()\n",
        "result = scraper.build(url, wanted_list)\n",
        "print(result)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blQg_83Vz-_Y"
      },
      "source": [
        "Question 3 (10 points). Understand the data quality: Search a second hand dataset (any dataset) from kaggle or other websites. Describe the data quality problem of the dataset and explain your strtegy to clean the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InRaAkaRz-_Z"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "'''\n",
        "\n",
        "Please write you answer here:\n",
        "\n",
        "\n",
        "I am taking a dataset from kaggle website. In this dataset the main thing is we need to import the packages and then reading that into a DataFrame.\n",
        "\n",
        "-> I considered a text classification data from Kaggle. Dataset: Link: https://github.com/revanthch/INFO-5502/blob/main/supermarket_sales%20-%20Sheet1.csv\n",
        "\n",
        "The dataset consists of two attributes which is displaying Entity of children.\n",
        "The message is of two types Median or 95 percentile\n",
        "Steps to Clean the Text classification data:\n",
        "\n",
        "Firstly all the elements must be in upper case or lower case.\n",
        "\n",
        "Then we need to remove all the data which contains stop.\n",
        "\n",
        "Special characters and unwanted spaces from the data should be removed.\n",
        "The data should Perform stemming and Lemmatization.\n",
        "Now we need to Check if there is any imbalances in the data\n",
        "If there is any imbalance, we use SMOTE technique to balance the data by performing over sampling and under sampling\n",
        "Then, we can build a classifier model to classify the data as Median or 95 percentile\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FJnbuq2m2wry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "scrap =  pd.DataFrame()\n",
        "# we iterate through both datasets \n",
        "for dataset in ['TwADR-L', 'AskAPatient']:\n",
        "    result = {}\n",
        "    y = pd.DataFrame()\n",
        "    for filepath in glob.glob('/content/drive/MyDrive/datasets/{}/*.txt'.format(dataset)):\n",
        "        x = pd.read_csv(filepath, sep = \"\\t\", header = None, encoding= 'unicode_escape')\n",
        "        y = y.append(data)\n",
        "    # we create combined dataframe by combining Each dataset text files.\n",
        "    y = y.reset_index(drop=True)\n",
        "    y['phrase_label'] = y[1] + \" \" + y[2]\n",
        "    y.columns = ['id', 'labels', 'phrases', 'phrase-label']\n",
        "    # type casting to str\n",
        "    y = y.astype({\"id\": str})\n",
        "    # converting all values to lower case\n",
        "    for columns in y.columns:\n",
        "        y[columns] = y[columns].str.lower() \n",
        "\n",
        "    # dropping duplicates\n",
        "    y = y.drop_duplicates('phrase-label')\n",
        "\n",
        "    # storing results of the table in dictionary\n",
        "    result['Unique_phrases'] = len(y['phrases'].unique())\n",
        "    result['Unique_labels'] = len(y['labels'].unique())\n",
        "    result['Unique_phrase_label_pairs'] = y.shape[0]\n",
        "    y1 = pd.DataFrame(y['phrases'].value_counts())\n",
        "    result['Phrases with multiple labels'] = y1[y1['phrases'] > 1].shape[0]\n",
        "    result['Minimum examples per label'] = y['labels'].value_counts().values.min()\n",
        "    result['Maximum examples per label'] = y['labels'].value_counts().values.max()\n",
        "    result['Average examples per label'] = round(y['labels'].value_counts().mean(), 2)\n",
        "    scrap = scrap.append(result, ignore_index=True)\n",
        "\n",
        "\n",
        "scrap = scrap.T\n",
        "scrap = scrap.astype({0: int, 1: int})\n",
        "scrap.columns = ['TwADR-L', 'AskAPatient']\n",
        "\n",
        "scrap"
      ],
      "metadata": {
        "id": "Jjci12LQ2x1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "sample = pd.DataFrame()\n",
        "\n",
        "for dataset in ['TwADR-L', 'AskAPatient']:   #two datasets\n",
        "    sto = pd.DataFrame()\n",
        "    result = {}\n",
        "    for filepath in glob.glob('/content/drive/MyDrive/datasets/{}/*.txt'.format(dataset)):\n",
        "        #printing the file path\n",
        "        data = pd.read_csv(filepath, sep = \"\\t\", header = None, encoding= 'unicode_escape')\n",
        "        sto = sto.append(data)\n",
        "    # combining dataset by text files.\n",
        "    sto = sto.reset_index(drop=True)\n",
        "    sto['phrase_label'] = sto[1] + \" \" + sto[2]\n",
        "    sto.columns = ['id', 'labels', 'phrases', 'phrase-label']\n",
        "    #printing the shape\n",
        "    sto = sto.astype({\"id\": str})\n",
        "    # converting elements into lower case\n",
        "    for columns in y.columns:\n",
        "        sto[columns] = sto[columns].str.lower() \n",
        "\n",
        "    # removing duplicates \n",
        "    y = y.drop_duplicates('phrase-label')\n",
        "\n",
        "    index_list = []\n",
        "    for i in range(y.shape[0]):\n",
        "        if y['labels'].value_counts()[y.iloc[i]['labels']] < 5:\n",
        "            index_list.append(i)\n",
        "\n",
        "    # dropping the labels which have less than count of 5.\n",
        "    y.drop(y.index[index_list], inplace=True)\n",
        "    result = {}\n",
        "    # storing the results\n",
        "    result['Unique_phrases'] = len(y['phrases'].unique())\n",
        "    result['Unique_labels'] = len(y['labels'].unique())\n",
        "    result['Unique_phrase_label_pairs'] = y.shape[0]\n",
        "    output = pd.DataFrame(y['phrases'].value_counts())\n",
        "    result['Phrases with multiple labels'] = z[y1['phrases'] > 1].shape[0]\n",
        "    result['Minimum examples per label'] = y['labels'].value_counts().values.min()\n",
        "    result['Maximum examples per label'] = y['labels'].value_counts().values.max()\n",
        "    result['Average examples per label'] = round(y['labels'].value_counts().mean(), 2)\n",
        "    # appending the stored values to dataframe\n",
        "    sample = sample.append(result, ignore_index=True)\n",
        "\n",
        "sample = sample.T\n",
        "sample.columns = ['TwADR-L', 'AskAPatient']\n",
        "\n",
        "sample"
      ],
      "metadata": {
        "id": "_rtgnrnA25Xw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "5c6a7f839effdddf35473dacbb9a8184e57ebbba5133cab03b12e4f28d4f0d0f"
      }
    },
    "colab": {
      "name": "lab_assignment_03.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}